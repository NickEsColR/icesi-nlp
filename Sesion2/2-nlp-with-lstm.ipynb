{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4efb44e",
   "metadata": {},
   "source": [
    "# NLP con Long-Short Term Memory (LSTM)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion2/2-nlp-with-lstm.ipynb)\n",
    "\n",
    "En este notebook implementaremos un clasificador de noticias en español utilizando la arquitectura de red LSTM. La idea es tener un punto de referencia para comparar cuando observemos la parte de transformers, por lo que utilizaremos el mismo dataset y tarea de ejemplo. Utilizarémos las utilidades de tokenización de huggingface transformers para ayudarnos con esta tarea.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n",
    "- [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory#fullTextFileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936855e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34267/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2013edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df40a98",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "Este es un dataset pequeño de articulos de noticias en idioma español con sus respectivas categorías. El dataset está disponible en el HuggingFace Hub y puede ser fácilmente descargado con la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b91601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 1.14kB [00:00, 1.31MB/s]\n",
      "Downloading data: 100%|██████████| 44.8M/44.8M [00:10<00:00, 4.14MB/s]\n",
      "Generating train split: 100%|██████████| 10200/10200 [00:00<00:00, 23599.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['language', 'category', 'newspaper', 'hash', 'text'],\n",
       "    num_rows: 10200\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "dataset = load_dataset('MarcOrfilaCarreras/spanish-news', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a53ebd",
   "metadata": {},
   "source": [
    "Observemos uno de sus registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14abea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'es',\n",
       " 'category': 'play',\n",
       " 'newspaper': 'de_lector_a_lector',\n",
       " 'hash': 'b387bc0a5ad68524c8aa5da489555ca41d5a3575',\n",
       " 'text': 'El coraje de ser, de Mónica Cavallé, la aventura del autoconocimiento filosófico.Todos experimentamos momentos de plenitud vinculados a la expresión directa y auténtica de nosotros mismos: momentos de contemplación de la belleza del mundo en que nuestros sentidos se abren como si lo vieran por primera vez, de intimidad y comunión con otro ser humano, de fluidez creativa, de expresión confiada y libre… Estos momentos permiten intuir lo que puede ser una vida en la que no meramente se existe, sino en la que se vive en todo el sentido de esta palabra.Esta vida solo es posible cuando sabemos quiénes somos, cuando nos conocemos a nosotros mismos de modo experiencial: no cuando nos llenamos de ideas sobre nosotros, sino cuando nos asentamos en nuestro ser real, más allá de nuestras defensas, máscaras y falsos yoes.El coraje de ser es una invitación a adentrarnos de forma práctica en el camino del autoconocimiento sapiencial y, más ampliamente, en la vida filosófica. Busca inspirar y acompañar en la apasionante aventura de desnudarnos, reconocer nuestra vulnerabilidad, para poder vernos y ser llenados. Solo esta desnudez lúcida da paso a una vida creativa y verdadera; una vida que no solo es una bendición para nosotros mismos, sino también para los demás y para el mundo.Mónica Cavallé es doctora en Filosofía por la Universidad Complutense de Madrid y máster en Ciencias de las Religiones. Ha sido profesora de Filosofía Práctica y durante varios años ha coordinado en la Universidad Complutense de Madrid los seminarios de Introducción Filosófica al Hinduismo y al Budismo.Trabaja como filósofa, asesora y dirige la Escuela de Filosofía Sapiencial. Entre sus obras escritas destacan La sabiduría recobrada, El arte de ser, y\\xa0La sabiduría de la no-dualidad, publicadas por Kairós.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2676",
   "metadata": {},
   "source": [
    "Para los efectos de esta tarea, nos servirán el texto y la categoría naturalmente.\n",
    "\n",
    "A manera general, observemos que tan largos o cortos tienden a ser los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6642761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto más corto: 501\n",
      "Texto más largo: 204324\n",
      "Longitud promedio: 4218.154509803921\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(row['text']) for row in dataset]\n",
    "print(f\"Texto más corto: {min(text_lengths)}\")\n",
    "print(f\"Texto más largo: {max(text_lengths)}\")\n",
    "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19905224",
   "metadata": {},
   "source": [
    "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 2000 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos.\n",
    "\n",
    "## Definiendo el Tokenizer\n",
    "\n",
    "Ahora, vamos a definir el tokenizer para nuestra tarea. Para ahorrarnos tiempo, vamos a entrenar uno basado en gpt2, pero ajustandolo a nuestro dataset. Para ello, debemos seleccionar una muestra representativa de nuestro dataset, como no es muy grande, casi que podemos usarlo todo. Luego, debemos definir el tamaño del vocabulario, es decir, cuantos tokens únicos queremos soportar en nuestro tokenizador. Para que un modelo de lenguaje funcione moderadamente bien para una tarea de clasificación, considerando el tamaño de nuestro corpus, deberíamos definir unos 50 mil tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39fb654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 159.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "\n",
    "length = 10000\n",
    "iter_dataset = iter(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "def batch_iterator(batch_size: int = 10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['text'] for _ in range(batch_size)]\n",
    "\n",
    "spanish_news_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50000, initial_alphabet=base_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ce90e",
   "metadata": {},
   "source": [
    "Exploremos ahora el tokenizador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71960c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 50000 tokens\n",
      "Primeros 15 tokens:\n",
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.']\n",
      "15 tokens de en medio:\n",
      "['pul', 'ece', ' Car', ' op', ' hech', 'ome', ' Ar', ' cen', 'ensa', 'ismo', 'ós', 'ord', 'at', '”.', 'lec']\n",
      "Últimos 15 tokens:\n",
      "['yang', 'zidina', 'zambique', '\\x96', ' ud', ' �', 'deado', ' each', ' eches', ' eternidad', ' eferentes', ' eBook', ' deple', ' dedu', ' deduce']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(spanish_news_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(f\"Vocabulario: {spanish_news_tokenizer.vocab_size} tokens\")\n",
    "print(\"Primeros 15 tokens:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[:15]])\n",
    "print(\"15 tokens de en medio:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[1000:1015]])\n",
    "print(\"Últimos 15 tokens:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0668c",
   "metadata": {},
   "source": [
    "Vemos que los primeros tokens corresponden a caracteres especiales y puntiación. Luego en el medio tenemos una combinación entre palabras completas y cortadas, el tokenizador se encarga de encontrar las frecuencias más comunes y asi partir las palabras por aquellas partes que tienden a repetirse mas. Esto es muy útil para trabajar con modelos de lenguaje ya que el modelo se vuelve robusto a diferentes ramificaciones de palabras e incluso a errores de tipografía. Finalmente, al final, vemos que tenemos más palabras cortadas y palabras muy especiales. Algo importante aquí es que podamos ver que los tokens tienen sentido con respecto a nuestro corpus.\n",
    "\n",
    "Ahora veamos como convierte el tokenizador una oración muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e68c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [72, 1086, 1039, 1, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_news_tokenizer.pad_token = '[PAD]'\n",
    "spanish_news_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e6557",
   "metadata": {},
   "source": [
    "Lo que obtenemos de vuelta son los ids de cada token según el vocabulario. Ahora algo importante que notamos aquí es el *padding*, durante el entrenamiento, queremos que las secuencias sean de tamaño fijo, para asi operar comodamente con matrices. Pero ya vimos que no todos los textos tienen la misma longitud. Entonces que hacer? para los que son más largos que una longitud dada simplemente cortamos, pero para los que son más cortos, debemos *rellenar* lo faltante con un *token especial de relleno o padding*. Y es justo lo que definimos allí, cuando la cadena es inferior a 8 **tokens**, entonces debemos hacer padding hasta que se cumplan los 8.\n",
    "\n",
    "Ahora, notemos que \"hola mundo!\" son 2 palabras, 9 letras, 1 espacio y 1 simbolo para un total de 11 caracteres, pero vemos que el resultado son 4 tokens y el padding. Esto es trabajo del tokenizador. Cuando lo entrenamos con nuestro corpus, el tokenizador computó las frecuencias de palabras y sus partes, tal como vimos arriba, entonces, estos tokens juntos forman la frase original, observemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a812a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'ola', 'Ġmundo', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_news_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length').tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18e433",
   "metadata": {},
   "source": [
    "Claramente vemos los 4 tokens como cadenas independientes.\n",
    "\n",
    "### Definiendo el dataset de pytorch\n",
    "Ahora podemos proceder a definir el dataset. Esto debería ser muy sencillo dado que nuestro dataset es pequeño y ya tenemos el tokenizador listo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca70e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpanishNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = '[PAD]'\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        # Definimos estos dos mapas para facilitarnos la tarea\n",
    "        # de traducir de nombres de categoría a ids de categoría.\n",
    "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
    "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
    "        self.num_classes = len(self.id_2_class_map)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
    "        y = self.class_2_id_map[y]\n",
    "        data = {k: torch.tensor(v) for k, v in self.tokenizer(text, max_length=self.seq_length, truncation=True, padding='max_length').items()}\n",
    "        data['y'] = torch.tensor(y)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de3493",
   "metadata": {},
   "source": [
    "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 2048 **tokens**. Que según nuestra intuición arriba, debería ser suficiente para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15200ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 \n",
    "spanish_news_dataset = SpanishNewsDataset(spanish_news_tokenizer, dataset, seq_length=max_len)\n",
    "assert len(spanish_news_dataset) == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d4b0b",
   "metadata": {},
   "source": [
    "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7e9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4 if not IN_COLAB else 12\n",
    "train_dataset, val_dataset, test_dataset = random_split(spanish_news_dataset, lengths=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac44796",
   "metadata": {},
   "source": [
    "## Definición del modelo LSTM\n",
    "\n",
    "Ahora vamos a configurar un módulo pytorch simple para este problema. Vamos ha utilizar los embeddings, que vendrían siendo los vectores de palabra. Pytorch nos ofrece una capa con la que directamente podemos entrenarlos a partir de los token ids obtenidos. El resto consistirá en invocar una capa LSTM seguida de una capa densa para la clasificación.\n",
    "\n",
    "Recordemos que las redes recurrentes como las LSTM por diseño enlazan todas las dimensiones del vector de entrada, formando así la secuencia, la estructura natural que necesitamos representar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38eb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        return hidden[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f68240",
   "metadata": {},
   "source": [
    "### Definición del clasificador\n",
    "\n",
    "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
    "\n",
    "- La tokenización, tal como la definimos anteriormente.\n",
    "- El bloque LSTM, que acabamos de decinir.\n",
    "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
    "\n",
    "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d724f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | lstm       | LSTMBlock          | 13.1 M | train\n",
      "1 | classifier | Sequential         | 200 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "13.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.3 M    Total params\n",
      "53.321    Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2040/2040 [02:31<00:00, 13.50it/s, v_num=1, val-loss=2.340, val-acc=0.158, train-loss=2.280, train-acc=0.181]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2040/2040 [02:31<00:00, 13.47it/s, v_num=1, val-loss=2.340, val-acc=0.158, train-loss=2.280, train-acc=0.181]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SpanishNewsClassifierWithLSTM(LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_classes: int, emb_dim: int, hidden_dim: int = 128):\n",
    "        super(SpanishNewsClassifierWithLSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = LSTMBlock(vocab_size, emb_dim, hidden_dim, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.lstm(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        self.test_acc(y_hat, y)\n",
    "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        x = batch['input_ids']\n",
    "        return self(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "model = SpanishNewsClassifierWithLSTM(vocab_size=spanish_news_tokenizer.vocab_size, num_classes=spanish_news_dataset.num_classes, emb_dim=256)\n",
    "\n",
    "tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n",
    "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
    "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7312d7c",
   "metadata": {},
   "source": [
    "Observemos el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65f9b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6dc01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cec7e82055e454e5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cec7e82055e454e5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd09d6",
   "metadata": {},
   "source": [
    "Y como es de esperarse, realizaremos la validación contra el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d52f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 255/255 [00:02<00:00, 119.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.15980392694473267    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15980392694473267   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-acc': 0.15980392694473267}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d232",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1c989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 255/255 [00:01<00:00, 130.29it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "predictions = torch.argmax(predictions, dim=-1)\n",
    "predictions = [spanish_news_dataset.id_2_class_map[pred] for pred in predictions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bdacb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1709 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categoría</th>\n",
       "      <th>predicción</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>TAG Heuer acaba de anunciar una curiosa y pecu...</td>\n",
       "      <td>[27693, 46981, 3747, 259, 11280, 347, 17325, 2...</td>\n",
       "      <td>[TAG, ĠHeuer, Ġacaba, Ġde, Ġanunciar, Ġuna, Ġc...</td>\n",
       "      <td>tech</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>Decía la mitología que el Ave Fénix es aquel q...</td>\n",
       "      <td>[36, 4961, 280, 20555, 288, 289, 348, 710, 456...</td>\n",
       "      <td>[D, ecÃŃa, Ġla, ĠmitologÃŃa, Ġque, Ġel, ĠA, ve...</td>\n",
       "      <td>sport</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>Un año más, el concurso que elige a la mejor c...</td>\n",
       "      <td>[2183, 795, 383, 12, 289, 9091, 288, 22358, 26...</td>\n",
       "      <td>[Un, ĠaÃ±o, ĠmÃ¡s, ,, Ġel, Ġconcurso, Ġque, Ġe...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>El pasado mes de febrero, el inversor George S...</td>\n",
       "      <td>[544, 1145, 946, 259, 1937, 12, 289, 23175, 96...</td>\n",
       "      <td>[El, Ġpasado, Ġmes, Ġde, Ġfebrero, ,, Ġel, Ġin...</td>\n",
       "      <td>tech</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>Repartirá un dividendo complementario de 0,09 ...</td>\n",
       "      <td>[4716, 493, 44812, 297, 14285, 28343, 259, 165...</td>\n",
       "      <td>[Re, par, tirÃ¡, Ġun, Ġdividendo, Ġcomplementa...</td>\n",
       "      <td>economy</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>Volvo sigue siendo noticia en los últimos días...</td>\n",
       "      <td>[42631, 1944, 1397, 5268, 279, 313, 1664, 1253...</td>\n",
       "      <td>[Volvo, Ġsigue, Ġsiendo, Ġnoticia, Ġen, Ġlos, ...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>Cada cuatro años el calendario se ve alterado ...</td>\n",
       "      <td>[10183, 1552, 640, 289, 6394, 309, 885, 34585,...</td>\n",
       "      <td>[Cada, Ġcuatro, ĠaÃ±os, Ġel, Ġcalendario, Ġse,...</td>\n",
       "      <td>economy</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>Aníbal Tortoriello, diputado nacional de Junto...</td>\n",
       "      <td>[2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...</td>\n",
       "      <td>[An, ÃŃ, bal, ĠTor, tor, iel, lo, ,, Ġdiputado...</td>\n",
       "      <td>politics</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>Las lesiones son una parte fundamental en el d...</td>\n",
       "      <td>[1492, 4361, 558, 347, 769, 3250, 279, 289, 59...</td>\n",
       "      <td>[Las, Ġlesiones, Ġson, Ġuna, Ġparte, Ġfundamen...</td>\n",
       "      <td>sport</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Era cuestión de tiempo que 'Barbie' siguiera r...</td>\n",
       "      <td>[17838, 3850, 259, 903, 288, 750, 11492, 7, 44...</td>\n",
       "      <td>[Era, ĠcuestiÃ³n, Ġde, Ġtiempo, Ġque, Ġ', Barb...</td>\n",
       "      <td>play</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>Fuente de la imagen, APEl atún azul constituye...</td>\n",
       "      <td>[3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...</td>\n",
       "      <td>[Fuente, Ġde, Ġla, Ġimagen, ,, ĠAP, El, ĠatÃºn...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>Esta vez había que restregarse los ojos porque...</td>\n",
       "      <td>[3025, 870, 1384, 288, 460, 482, 9569, 313, 57...</td>\n",
       "      <td>[Esta, Ġvez, ĠhabÃŃa, Ġque, Ġres, tre, garse, ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>La física cuántica, ese reino extraño y maravi...</td>\n",
       "      <td>[606, 4222, 19193, 12, 1082, 12817, 10825, 290...</td>\n",
       "      <td>[La, ĠfÃŃsica, ĠcuÃ¡ntica, ,, Ġese, Ġreino, Ġe...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Hoy día no hay nadie que le haga sombra a DJI,...</td>\n",
       "      <td>[7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...</td>\n",
       "      <td>[Hoy, ĠdÃŃa, Ġno, Ġhay, Ġnadie, Ġque, Ġle, Ġha...</td>\n",
       "      <td>tech</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>Tecnobit - Grupo Oesía amplía la familia de ra...</td>\n",
       "      <td>[27489, 17595, 864, 4483, 10768, 16645, 280, 2...</td>\n",
       "      <td>[Tec, nobit, Ġ-, ĠGrupo, ĠOesÃŃa, ĠamplÃŃa, Ġl...</td>\n",
       "      <td>military</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "1629  TAG Heuer acaba de anunciar una curiosa y pecu...   \n",
       "4186  Decía la mitología que el Ave Fénix es aquel q...   \n",
       "5676  Un año más, el concurso que elige a la mejor c...   \n",
       "1538  El pasado mes de febrero, el inversor George S...   \n",
       "9541  Repartirá un dividendo complementario de 0,09 ...   \n",
       "8633  Volvo sigue siendo noticia en los últimos días...   \n",
       "9722  Cada cuatro años el calendario se ve alterado ...   \n",
       "6129  Aníbal Tortoriello, diputado nacional de Junto...   \n",
       "3576  Las lesiones son una parte fundamental en el d...   \n",
       "338   Era cuestión de tiempo que 'Barbie' siguiera r...   \n",
       "2814  Fuente de la imagen, APEl atún azul constituye...   \n",
       "3686  Esta vez había que restregarse los ojos porque...   \n",
       "2952  La física cuántica, ese reino extraño y maravi...   \n",
       "1261  Hoy día no hay nadie que le haga sombra a DJI,...   \n",
       "7204  Tecnobit - Grupo Oesía amplía la familia de ra...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1629  [27693, 46981, 3747, 259, 11280, 347, 17325, 2...   \n",
       "4186  [36, 4961, 280, 20555, 288, 289, 348, 710, 456...   \n",
       "5676  [2183, 795, 383, 12, 289, 9091, 288, 22358, 26...   \n",
       "1538  [544, 1145, 946, 259, 1937, 12, 289, 23175, 96...   \n",
       "9541  [4716, 493, 44812, 297, 14285, 28343, 259, 165...   \n",
       "8633  [42631, 1944, 1397, 5268, 279, 313, 1664, 1253...   \n",
       "9722  [10183, 1552, 640, 289, 6394, 309, 885, 34585,...   \n",
       "6129  [2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...   \n",
       "3576  [1492, 4361, 558, 347, 769, 3250, 279, 289, 59...   \n",
       "338   [17838, 3850, 259, 903, 288, 750, 11492, 7, 44...   \n",
       "2814  [3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...   \n",
       "3686  [3025, 870, 1384, 288, 460, 482, 9569, 313, 57...   \n",
       "2952  [606, 4222, 19193, 12, 1082, 12817, 10825, 290...   \n",
       "1261  [7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...   \n",
       "7204  [27489, 17595, 864, 4483, 10768, 16645, 280, 2...   \n",
       "\n",
       "                                          tokens_string     categoría  \\\n",
       "1629  [TAG, ĠHeuer, Ġacaba, Ġde, Ġanunciar, Ġuna, Ġc...          tech   \n",
       "4186  [D, ecÃŃa, Ġla, ĠmitologÃŃa, Ġque, Ġel, ĠA, ve...         sport   \n",
       "5676  [Un, ĠaÃ±o, ĠmÃ¡s, ,, Ġel, Ġconcurso, Ġque, Ġe...  alimentation   \n",
       "1538  [El, Ġpasado, Ġmes, Ġde, Ġfebrero, ,, Ġel, Ġin...          tech   \n",
       "9541  [Re, par, tirÃ¡, Ġun, Ġdividendo, Ġcomplementa...       economy   \n",
       "8633  [Volvo, Ġsigue, Ġsiendo, Ġnoticia, Ġen, Ġlos, ...         motor   \n",
       "9722  [Cada, Ġcuatro, ĠaÃ±os, Ġel, Ġcalendario, Ġse,...       economy   \n",
       "6129  [An, ÃŃ, bal, ĠTor, tor, iel, lo, ,, Ġdiputado...      politics   \n",
       "3576  [Las, Ġlesiones, Ġson, Ġuna, Ġparte, Ġfundamen...         sport   \n",
       "338   [Era, ĠcuestiÃ³n, Ġde, Ġtiempo, Ġque, Ġ', Barb...          play   \n",
       "2814  [Fuente, Ġde, Ġla, Ġimagen, ,, ĠAP, El, ĠatÃºn...     astronomy   \n",
       "3686  [Esta, Ġvez, ĠhabÃŃa, Ġque, Ġres, tre, garse, ...         sport   \n",
       "2952  [La, ĠfÃŃsica, ĠcuÃ¡ntica, ,, Ġese, Ġreino, Ġe...     astronomy   \n",
       "1261  [Hoy, ĠdÃŃa, Ġno, Ġhay, Ġnadie, Ġque, Ġle, Ġha...          tech   \n",
       "7204  [Tec, nobit, Ġ-, ĠGrupo, ĠOesÃŃa, ĠamplÃŃa, Ġl...      military   \n",
       "\n",
       "     predicción  \n",
       "1629   military  \n",
       "4186   military  \n",
       "5676       play  \n",
       "1538   medicine  \n",
       "9541   medicine  \n",
       "8633       play  \n",
       "9722   politics  \n",
       "6129       play  \n",
       "3576   medicine  \n",
       "338        play  \n",
       "2814       play  \n",
       "3686       play  \n",
       "2952       play  \n",
       "1261      motor  \n",
       "7204   military  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_indices = test_dataset.indices\n",
    "df = pd.DataFrame(data={\n",
    "    \"texto\": dataset[test_indices]['text'],\n",
    "    \"tokens\": [spanish_news_tokenizer(v)['input_ids'] for v in dataset[test_indices]['text']],\n",
    "    \"categoría\": dataset[test_indices]['category'],\n",
    "    'predicción': predictions\n",
    "}, index=test_indices)\n",
    "\n",
    "df['tokens_string'] = df.tokens.apply(lambda t: spanish_news_tokenizer.convert_ids_to_tokens(t))\n",
    "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categoría\", \"predicción\"]]\n",
    "df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
    "    ]\n",
    ")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c5085d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categoría</th>\n",
       "      <th>predicción</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>TAG Heuer acaba de anunciar una curiosa y pecu...</td>\n",
       "      <td>[27693, 46981, 3747, 259, 11280, 347, 17325, 2...</td>\n",
       "      <td>[TAG, ĠHeuer, Ġacaba, Ġde, Ġanunciar, Ġuna, Ġc...</td>\n",
       "      <td>tech</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>Decía la mitología que el Ave Fénix es aquel q...</td>\n",
       "      <td>[36, 4961, 280, 20555, 288, 289, 348, 710, 456...</td>\n",
       "      <td>[D, ecÃŃa, Ġla, ĠmitologÃŃa, Ġque, Ġel, ĠA, ve...</td>\n",
       "      <td>sport</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>Un año más, el concurso que elige a la mejor c...</td>\n",
       "      <td>[2183, 795, 383, 12, 289, 9091, 288, 22358, 26...</td>\n",
       "      <td>[Un, ĠaÃ±o, ĠmÃ¡s, ,, Ġel, Ġconcurso, Ġque, Ġe...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>El pasado mes de febrero, el inversor George S...</td>\n",
       "      <td>[544, 1145, 946, 259, 1937, 12, 289, 23175, 96...</td>\n",
       "      <td>[El, Ġpasado, Ġmes, Ġde, Ġfebrero, ,, Ġel, Ġin...</td>\n",
       "      <td>tech</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>Repartirá un dividendo complementario de 0,09 ...</td>\n",
       "      <td>[4716, 493, 44812, 297, 14285, 28343, 259, 165...</td>\n",
       "      <td>[Re, par, tirÃ¡, Ġun, Ġdividendo, Ġcomplementa...</td>\n",
       "      <td>economy</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>Volvo sigue siendo noticia en los últimos días...</td>\n",
       "      <td>[42631, 1944, 1397, 5268, 279, 313, 1664, 1253...</td>\n",
       "      <td>[Volvo, Ġsigue, Ġsiendo, Ġnoticia, Ġen, Ġlos, ...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>Cada cuatro años el calendario se ve alterado ...</td>\n",
       "      <td>[10183, 1552, 640, 289, 6394, 309, 885, 34585,...</td>\n",
       "      <td>[Cada, Ġcuatro, ĠaÃ±os, Ġel, Ġcalendario, Ġse,...</td>\n",
       "      <td>economy</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>Aníbal Tortoriello, diputado nacional de Junto...</td>\n",
       "      <td>[2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...</td>\n",
       "      <td>[An, ÃŃ, bal, ĠTor, tor, iel, lo, ,, Ġdiputado...</td>\n",
       "      <td>politics</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>Las lesiones son una parte fundamental en el d...</td>\n",
       "      <td>[1492, 4361, 558, 347, 769, 3250, 279, 289, 59...</td>\n",
       "      <td>[Las, Ġlesiones, Ġson, Ġuna, Ġparte, Ġfundamen...</td>\n",
       "      <td>sport</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>Fuente de la imagen, APEl atún azul constituye...</td>\n",
       "      <td>[3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...</td>\n",
       "      <td>[Fuente, Ġde, Ġla, Ġimagen, ,, ĠAP, El, ĠatÃºn...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>Esta vez había que restregarse los ojos porque...</td>\n",
       "      <td>[3025, 870, 1384, 288, 460, 482, 9569, 313, 57...</td>\n",
       "      <td>[Esta, Ġvez, ĠhabÃŃa, Ġque, Ġres, tre, garse, ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>La física cuántica, ese reino extraño y maravi...</td>\n",
       "      <td>[606, 4222, 19193, 12, 1082, 12817, 10825, 290...</td>\n",
       "      <td>[La, ĠfÃŃsica, ĠcuÃ¡ntica, ,, Ġese, Ġreino, Ġe...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Hoy día no hay nadie que le haga sombra a DJI,...</td>\n",
       "      <td>[7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...</td>\n",
       "      <td>[Hoy, ĠdÃŃa, Ġno, Ġhay, Ġnadie, Ġque, Ġle, Ġha...</td>\n",
       "      <td>tech</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>No hay muchos obispos que puedan dirigirse a m...</td>\n",
       "      <td>[1454, 694, 1400, 7004, 288, 3542, 31653, 264,...</td>\n",
       "      <td>[No, Ġhay, Ġmuchos, Ġobispos, Ġque, Ġpuedan, Ġ...</td>\n",
       "      <td>religion</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8940</th>\n",
       "      <td>El pasado lunes 5 de febrero salió a la luz qu...</td>\n",
       "      <td>[544, 1145, 3908, 705, 259, 1937, 9098, 264, 2...</td>\n",
       "      <td>[El, Ġpasado, Ġlunes, Ġ5, Ġde, Ġfebrero, Ġsali...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "1629  TAG Heuer acaba de anunciar una curiosa y pecu...   \n",
       "4186  Decía la mitología que el Ave Fénix es aquel q...   \n",
       "5676  Un año más, el concurso que elige a la mejor c...   \n",
       "1538  El pasado mes de febrero, el inversor George S...   \n",
       "9541  Repartirá un dividendo complementario de 0,09 ...   \n",
       "8633  Volvo sigue siendo noticia en los últimos días...   \n",
       "9722  Cada cuatro años el calendario se ve alterado ...   \n",
       "6129  Aníbal Tortoriello, diputado nacional de Junto...   \n",
       "3576  Las lesiones son una parte fundamental en el d...   \n",
       "2814  Fuente de la imagen, APEl atún azul constituye...   \n",
       "3686  Esta vez había que restregarse los ojos porque...   \n",
       "2952  La física cuántica, ese reino extraño y maravi...   \n",
       "1261  Hoy día no hay nadie que le haga sombra a DJI,...   \n",
       "8260  No hay muchos obispos que puedan dirigirse a m...   \n",
       "8940  El pasado lunes 5 de febrero salió a la luz qu...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1629  [27693, 46981, 3747, 259, 11280, 347, 17325, 2...   \n",
       "4186  [36, 4961, 280, 20555, 288, 289, 348, 710, 456...   \n",
       "5676  [2183, 795, 383, 12, 289, 9091, 288, 22358, 26...   \n",
       "1538  [544, 1145, 946, 259, 1937, 12, 289, 23175, 96...   \n",
       "9541  [4716, 493, 44812, 297, 14285, 28343, 259, 165...   \n",
       "8633  [42631, 1944, 1397, 5268, 279, 313, 1664, 1253...   \n",
       "9722  [10183, 1552, 640, 289, 6394, 309, 885, 34585,...   \n",
       "6129  [2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...   \n",
       "3576  [1492, 4361, 558, 347, 769, 3250, 279, 289, 59...   \n",
       "2814  [3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...   \n",
       "3686  [3025, 870, 1384, 288, 460, 482, 9569, 313, 57...   \n",
       "2952  [606, 4222, 19193, 12, 1082, 12817, 10825, 290...   \n",
       "1261  [7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...   \n",
       "8260  [1454, 694, 1400, 7004, 288, 3542, 31653, 264,...   \n",
       "8940  [544, 1145, 3908, 705, 259, 1937, 9098, 264, 2...   \n",
       "\n",
       "                                          tokens_string     categoría  \\\n",
       "1629  [TAG, ĠHeuer, Ġacaba, Ġde, Ġanunciar, Ġuna, Ġc...          tech   \n",
       "4186  [D, ecÃŃa, Ġla, ĠmitologÃŃa, Ġque, Ġel, ĠA, ve...         sport   \n",
       "5676  [Un, ĠaÃ±o, ĠmÃ¡s, ,, Ġel, Ġconcurso, Ġque, Ġe...  alimentation   \n",
       "1538  [El, Ġpasado, Ġmes, Ġde, Ġfebrero, ,, Ġel, Ġin...          tech   \n",
       "9541  [Re, par, tirÃ¡, Ġun, Ġdividendo, Ġcomplementa...       economy   \n",
       "8633  [Volvo, Ġsigue, Ġsiendo, Ġnoticia, Ġen, Ġlos, ...         motor   \n",
       "9722  [Cada, Ġcuatro, ĠaÃ±os, Ġel, Ġcalendario, Ġse,...       economy   \n",
       "6129  [An, ÃŃ, bal, ĠTor, tor, iel, lo, ,, Ġdiputado...      politics   \n",
       "3576  [Las, Ġlesiones, Ġson, Ġuna, Ġparte, Ġfundamen...         sport   \n",
       "2814  [Fuente, Ġde, Ġla, Ġimagen, ,, ĠAP, El, ĠatÃºn...     astronomy   \n",
       "3686  [Esta, Ġvez, ĠhabÃŃa, Ġque, Ġres, tre, garse, ...         sport   \n",
       "2952  [La, ĠfÃŃsica, ĠcuÃ¡ntica, ,, Ġese, Ġreino, Ġe...     astronomy   \n",
       "1261  [Hoy, ĠdÃŃa, Ġno, Ġhay, Ġnadie, Ġque, Ġle, Ġha...          tech   \n",
       "8260  [No, Ġhay, Ġmuchos, Ġobispos, Ġque, Ġpuedan, Ġ...      religion   \n",
       "8940  [El, Ġpasado, Ġlunes, Ġ5, Ġde, Ġfebrero, Ġsali...         motor   \n",
       "\n",
       "     predicción  \n",
       "1629   military  \n",
       "4186   military  \n",
       "5676       play  \n",
       "1538   medicine  \n",
       "9541   medicine  \n",
       "8633       play  \n",
       "9722   politics  \n",
       "6129       play  \n",
       "3576   medicine  \n",
       "2814       play  \n",
       "3686       play  \n",
       "2952       play  \n",
       "1261      motor  \n",
       "8260   politics  \n",
       "8940       play  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = df[df['categoría'] != df['predicción']]\n",
    "errors.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88103e5e",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- En este caso tenemos una tarea de clasificación de texto de múltiples clases.\n",
    "- Estamos usando un bloque LSTM como featurizer, es decir lo usamos para extraer features de las secuencias de entrada con las cuales harémos predicciones luego.\n",
    "- Nótese que de las capas LSTM, solo nos interesa la última, ya que esta recupera todas las operaciones enalazadas anteriores.\n",
    "- Observamos que el modelo toma su tiempo en entrenar, esto es natural debido al diseño de las LSTM, donde por cada paso de tiempo se debe computar un gradiente, por lo que el computo es mucho mayor.\n",
    "- Los resultados de clasificación no son malos, pero tampoco son excelentes. Podemos hacerlo mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73526f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

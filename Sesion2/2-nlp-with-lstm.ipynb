{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4efb44e",
   "metadata": {},
   "source": [
    "# NLP con Long-Short Term Memory (LSTM)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion2/2-nlp-with-lstm.ipynb)\n",
    "\n",
    "En este notebook implementaremos un clasificador de noticias en español utilizando la arquitectura de red LSTM. La idea es tener un punto de referencia para comparar cuando observemos la parte de transformers, por lo que utilizaremos el mismo dataset y tarea de ejemplo. Utilizarémos las utilidades de tokenización de huggingface transformers para ayudarnos con esta tarea.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n",
    "- [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory#fullTextFileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2013edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df40a98",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "Este es un dataset pequeño de articulos de noticias en idioma español con sus respectivas categorías. El dataset está disponible en el HuggingFace Hub y puede ser fácilmente descargado con la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b91601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['language', 'category', 'newspaper', 'hash', 'text'],\n",
       "    num_rows: 10200\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "dataset = load_dataset('MarcOrfilaCarreras/spanish-news', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a53ebd",
   "metadata": {},
   "source": [
    "Observemos uno de sus registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14abea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'es',\n",
       " 'category': 'play',\n",
       " 'newspaper': 'de_lector_a_lector',\n",
       " 'hash': 'b387bc0a5ad68524c8aa5da489555ca41d5a3575',\n",
       " 'text': 'El coraje de ser, de Mónica Cavallé, la aventura del autoconocimiento filosófico.Todos experimentamos momentos de plenitud vinculados a la expresión directa y auténtica de nosotros mismos: momentos de contemplación de la belleza del mundo en que nuestros sentidos se abren como si lo vieran por primera vez, de intimidad y comunión con otro ser humano, de fluidez creativa, de expresión confiada y libre… Estos momentos permiten intuir lo que puede ser una vida en la que no meramente se existe, sino en la que se vive en todo el sentido de esta palabra.Esta vida solo es posible cuando sabemos quiénes somos, cuando nos conocemos a nosotros mismos de modo experiencial: no cuando nos llenamos de ideas sobre nosotros, sino cuando nos asentamos en nuestro ser real, más allá de nuestras defensas, máscaras y falsos yoes.El coraje de ser es una invitación a adentrarnos de forma práctica en el camino del autoconocimiento sapiencial y, más ampliamente, en la vida filosófica. Busca inspirar y acompañar en la apasionante aventura de desnudarnos, reconocer nuestra vulnerabilidad, para poder vernos y ser llenados. Solo esta desnudez lúcida da paso a una vida creativa y verdadera; una vida que no solo es una bendición para nosotros mismos, sino también para los demás y para el mundo.Mónica Cavallé es doctora en Filosofía por la Universidad Complutense de Madrid y máster en Ciencias de las Religiones. Ha sido profesora de Filosofía Práctica y durante varios años ha coordinado en la Universidad Complutense de Madrid los seminarios de Introducción Filosófica al Hinduismo y al Budismo.Trabaja como filósofa, asesora y dirige la Escuela de Filosofía Sapiencial. Entre sus obras escritas destacan La sabiduría recobrada, El arte de ser, y\\xa0La sabiduría de la no-dualidad, publicadas por Kairós.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2676",
   "metadata": {},
   "source": [
    "Para los efectos de esta tarea, nos servirán el texto y la categoría naturalmente.\n",
    "\n",
    "A manera general, observemos que tan largos o cortos tienden a ser los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6642761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto más corto: 501\n",
      "Texto más largo: 204324\n",
      "Longitud promedio: 4218.154509803921\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(row['text']) for row in dataset]\n",
    "print(f\"Texto más corto: {min(text_lengths)}\")\n",
    "print(f\"Texto más largo: {max(text_lengths)}\")\n",
    "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19905224",
   "metadata": {},
   "source": [
    "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 2000 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos.\n",
    "\n",
    "## Definiendo el Tokenizer\n",
    "\n",
    "Ahora, vamos a definir el tokenizer para nuestra tarea. Para mantener las cosas simples, vamos a mantener un conteo de palabras y vamos a hacer un corte hasta los primeros 50mil tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d840a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-záéíóúüñ]+\", \" \", text)\n",
    "    return text.strip().split()\n",
    "\n",
    "# Construimos el vocabulario a partir de conjunto de datos.\n",
    "token_counts = Counter()\n",
    "for text in dataset[\"text\"]:\n",
    "    token_counts.update(simple_tokenizer(text))\n",
    "\n",
    "# 50k-2 porque necesitamos reservar espacio para los dos tokens especiales\n",
    "top_n_tokens = list(token_counts.keys())[:50000-2]\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "for token in top_n_tokens:\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "def tokenize_text(text, max_length=50):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    ids = [vocab.get(tok, vocab[\"[UNK]\"]) for tok in tokens[:max_length]]\n",
    "    ids += [vocab[\"[PAD]\"]] * (max_length - len(ids))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ce90e",
   "metadata": {},
   "source": [
    "Exploremos ahora el tokenizador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f037f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 50000 tokens\n",
      "Primeros 15 tokens:\n",
      "['valladolid', 'misteriosa', 'es', 'el', 'título', 'del', 'nuevo', 'libro', 'que', 'acaba', 'de', 'publicar', 'la', 'editorial', 'almuzara']\n",
      "15 tokens de en medio:\n",
      "['trabajar', 'primero', 'seis', 'meses', 'guionista', 'antigua', 'city', 'tv', 'después', 'llamó', 'tres', 'medio', 'redactora', 'super', 'pop']\n",
      "Últimos 15 tokens:\n",
      "['risco', 'infecciosas', 'amalia', 'simoni', 'camagüey', 'instructor', 'proteinuria', 'disfunción', 'endotelial', 'diabéticos', 'amlodipine', 'bloqueador', 'aml', 'hctz', 'tolerada']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulario: {len(vocab)} tokens\")\n",
    "print(\"Primeros 15 tokens:\")\n",
    "print(f\"{top_n_tokens[:15]}\")\n",
    "print(\"15 tokens de en medio:\")\n",
    "print(f\"{top_n_tokens[1000:1015]}\")\n",
    "print(\"Últimos 15 tokens:\")\n",
    "print(f\"{top_n_tokens[-15:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0668c",
   "metadata": {},
   "source": [
    "Esta forma de exploración es para darnos una idea de las palabras más utilizadas en el corpus y nos dará un indicio de si la tokenización es adecuada o no. Vemos que tenemos algunos stop words, como artículos (el, la) y conectores (del, que). Para una tarea de clasificación de texto podríamos prescindir de estos pero para facilitar las cosas y ya que los demás tokens lucen bien, podemos preservarlos.\n",
    "\n",
    "Ahora veamos como convierte el tokenizador una oración muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4e68c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29340, 157, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenize_text(\"hola mundo\", max_length=8)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e6557",
   "metadata": {},
   "source": [
    "Lo que obtenemos de vuelta son los ids de cada token según el vocabulario. Ahora algo importante que notamos aquí es el *padding*, durante el entrenamiento, queremos que las secuencias sean de tamaño fijo, para asi operar comodamente con matrices. Pero ya vimos que no todos los textos tienen la misma longitud. Entonces que hacer? para los que son más largos que una longitud dada simplemente cortamos, pero para los que son más cortos, debemos *rellenar* lo faltante con un *token especial de relleno o padding*. Y es justo lo que definimos allí, cuando la cadena es inferior a 8 **tokens**, entonces debemos hacer padding hasta que se cumplan los 8.\n",
    "\n",
    "Si queremos saber a que token exactamente hacen referencia estos ids, simplemente revisamos el vocabulario que hemos construido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5049aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'mundo', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_2_token = {v: k for k, v in vocab.items()}\n",
    "[id_2_token[token] for token in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18e433",
   "metadata": {},
   "source": [
    "Claramente vemos los 3 tokens como cadenas independientes (el padding se considera un token independiente).\n",
    "\n",
    "### Definiendo el dataset de pytorch\n",
    "Ahora podemos proceder a definir el dataset. Esto debería ser muy sencillo dado que nuestro dataset es pequeño y ya tenemos el tokenizador listo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca70e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpanishNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        # Definimos estos dos mapas para facilitarnos la tarea\n",
    "        # de traducir de nombres de categoría a ids de categoría.\n",
    "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
    "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
    "        self.num_classes = len(self.id_2_class_map)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
    "        y = self.class_2_id_map[y]\n",
    "        data = {'input_ids': torch.tensor(self.tokenizer(text, max_length=self.seq_length))}\n",
    "        data['y'] = torch.tensor(y)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de3493",
   "metadata": {},
   "source": [
    "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 2048 **tokens**. Que según nuestra intuición arriba, debería ser suficiente para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15200ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 \n",
    "spanish_news_dataset = SpanishNewsDataset(tokenize_text, dataset, seq_length=max_len)\n",
    "assert len(spanish_news_dataset) == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d4b0b",
   "metadata": {},
   "source": [
    "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7e9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4 if not IN_COLAB else 12\n",
    "train_dataset, val_dataset, test_dataset = random_split(spanish_news_dataset, lengths=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac44796",
   "metadata": {},
   "source": [
    "## Definición del modelo LSTM\n",
    "\n",
    "Ahora vamos a configurar un módulo pytorch simple para este problema. Vamos ha utilizar los embeddings, que vendrían siendo los vectores de palabra. Pytorch nos ofrece una capa con la que directamente podemos entrenarlos a partir de los token ids obtenidos. El resto consistirá en invocar una capa LSTM seguida de una capa densa para la clasificación.\n",
    "\n",
    "Recordemos que las redes recurrentes como las LSTM por diseño enlazan todas las dimensiones del vector de entrada, formando así la secuencia, la estructura natural que necesitamos representar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d38eb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        return hidden[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f68240",
   "metadata": {},
   "source": [
    "### Definición del clasificador\n",
    "\n",
    "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
    "\n",
    "- La tokenización, tal como la definimos anteriormente.\n",
    "- El bloque LSTM, que acabamos de decinir.\n",
    "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
    "\n",
    "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d724f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | lstm       | LSTMBlock          | 13.1 M | train\n",
      "1 | classifier | Sequential         | 200 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "13.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.3 M    Total params\n",
      "53.322    Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2040/2040 [00:25<00:00, 79.38it/s, v_num=0, val-loss=0.525, val-acc=0.907, train-loss=0.0621, train-acc=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2040/2040 [00:26<00:00, 78.38it/s, v_num=0, val-loss=0.525, val-acc=0.907, train-loss=0.0621, train-acc=0.985]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SpanishNewsClassifierWithLSTM(LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_classes: int, emb_dim: int, hidden_dim: int = 128):\n",
    "        super(SpanishNewsClassifierWithLSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = LSTMBlock(vocab_size, emb_dim, hidden_dim, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.lstm(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        # print(f\"\\nbatch-idx: {batch_idx}\")\n",
    "        # print(f\"shape of x: {x.shape}\")\n",
    "        # print(torch.max(x, dim=0))\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        self.test_acc(y_hat, y)\n",
    "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        x = batch['input_ids']\n",
    "        return self(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "model = SpanishNewsClassifierWithLSTM(vocab_size=len(vocab) + 1, num_classes=spanish_news_dataset.num_classes, emb_dim=256)\n",
    "\n",
    "tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n",
    "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
    "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7312d7c",
   "metadata": {},
   "source": [
    "Observemos el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65f9b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e6dc01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-42fd403edfc8a0e6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-42fd403edfc8a0e6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd09d6",
   "metadata": {},
   "source": [
    "Y como es de esperarse, realizaremos la validación contra el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5d52f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 255/255 [00:01<00:00, 237.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9029411673545837     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9029411673545837    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-acc': 0.9029411673545837}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d232",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1c989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 255/255 [00:01<00:00, 219.44it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "predictions = torch.argmax(predictions, dim=-1)\n",
    "predictions = [spanish_news_dataset.id_2_class_map[pred] for pred in predictions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eec7e50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734    [5, 3270, 1058, 102, 1737, 4007, 240, 834, 10,...\n",
       "1905    [34012, 12, 1, 1, 36, 46, 703, 58, 1, 36, 1652...\n",
       "3049    [5, 14608, 14, 2248, 10396, 12, 2641, 10, 246,...\n",
       "1772    [1, 54, 14, 1, 40, 554, 12, 102, 1, 1652, 12, ...\n",
       "6980    [1, 11, 12, 23970, 58, 14, 27045, 12, 27768, 1...\n",
       "                              ...                        \n",
       "2655    [20589, 27004, 572, 1101, 34564, 1, 1183, 452,...\n",
       "9968    [1, 36, 42868, 850, 9373, 45, 1548, 269, 3270,...\n",
       "6426    [130, 12, 1057, 1005, 1010, 12, 10, 17, 1, 14,...\n",
       "1616    [493, 5, 3636, 4, 809, 12, 34, 3599, 130, 811,...\n",
       "8861    [5, 6030, 12, 1, 269, 1, 5, 8762, 12, 25, 1, 2...\n",
       "Name: tokens, Length: 1020, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bdacb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categoría</th>\n",
       "      <th>predicción</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>El Gobierno tiene un plan alternativo para ase...</td>\n",
       "      <td>[5, 3270, 1058, 102, 1737, 4007, 240, 834, 10,...</td>\n",
       "      <td>el gobierno tiene un plan alternativo para ase...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>Prevalencia de hipotiroidismo subclínico y su ...</td>\n",
       "      <td>[34012, 12, 1, 1, 36, 46, 703, 58, 1, 36, 1652...</td>\n",
       "      <td>prevalencia de [UNK] [UNK] y su relación con [...</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>El Sol, la fuente inagotable de energía que da...</td>\n",
       "      <td>[5, 14608, 14, 2248, 10396, 12, 2641, 10, 246,...</td>\n",
       "      <td>el sol la fuente inagotable de energía que da ...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>Mondor en la mama. A propósito de un casoLa en...</td>\n",
       "      <td>[1, 54, 14, 1, 40, 554, 12, 102, 1, 1652, 12, ...</td>\n",
       "      <td>[UNK] en la [UNK] a propósito de un [UNK] enfe...</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>Navantia acaba de firmar con la Marina de Noru...</td>\n",
       "      <td>[1, 11, 12, 23970, 58, 14, 27045, 12, 27768, 1...</td>\n",
       "      <td>[UNK] acaba de firmar con la marina de noruega...</td>\n",
       "      <td>military</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>Tienen una función parecida a los relojes o la...</td>\n",
       "      <td>[440, 19, 2427, 14721, 40, 34, 14066, 96, 77, ...</td>\n",
       "      <td>tienen una función parecida a los relojes o la...</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>En última instancia, cuando se apagan las luce...</td>\n",
       "      <td>[54, 1177, 5104, 206, 17, 16158, 77, 3867, 12,...</td>\n",
       "      <td>en última instancia cuando se apagan las luces...</td>\n",
       "      <td>play</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>La compañía francesa Escape International ha p...</td>\n",
       "      <td>[14, 1151, 2772, 7008, 29665, 260, 1048, 54, 1...</td>\n",
       "      <td>la compañía francesa escape international ha p...</td>\n",
       "      <td>military</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6763</th>\n",
       "      <td>El Gobierno busca que Bruselas apruebe un impu...</td>\n",
       "      <td>[5, 3270, 232, 10, 14032, 36279, 102, 15003, 2...</td>\n",
       "      <td>el gobierno busca que bruselas apruebe un impu...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>comentarios 44Cybertron, el planeta de los Tra...</td>\n",
       "      <td>[7493, 1, 5, 3124, 12, 34, 8114, 5, 29619, 12,...</td>\n",
       "      <td>comentarios [UNK] el planeta de los transforme...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>La palabra bienquerencia es poco usada, pero e...</td>\n",
       "      <td>[14, 203, 1, 4, 610, 32326, 403, 198, 11659, 2...</td>\n",
       "      <td>la palabra [UNK] es poco usada pero existe ind...</td>\n",
       "      <td>religion</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>Segunda victoria en el WRC para el finlandés E...</td>\n",
       "      <td>[1090, 3982, 54, 5, 1, 240, 5, 19053, 1, 1, 10...</td>\n",
       "      <td>segunda victoria en el [UNK] para el finlandés...</td>\n",
       "      <td>motor</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>No hay un equipo más desgraciado que el Almerí...</td>\n",
       "      <td>[196, 124, 102, 4877, 130, 20027, 10, 5, 23674...</td>\n",
       "      <td>no hay un equipo más desgraciado que el almerí...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659</th>\n",
       "      <td>Más allá de las cápsulas de café hay un mundo ...</td>\n",
       "      <td>[130, 219, 12, 77, 32819, 12, 10027, 124, 102,...</td>\n",
       "      <td>más allá de las cápsulas de café hay un mundo ...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>alimentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>Evolucion de la vida sexual de la mujer. Fisio...</td>\n",
       "      <td>[1, 12, 14, 195, 7773, 12, 14, 749, 1, 12, 14,...</td>\n",
       "      <td>[UNK] de la vida sexual de la mujer [UNK] de l...</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "6734  El Gobierno tiene un plan alternativo para ase...   \n",
       "1905  Prevalencia de hipotiroidismo subclínico y su ...   \n",
       "3049  El Sol, la fuente inagotable de energía que da...   \n",
       "1772  Mondor en la mama. A propósito de un casoLa en...   \n",
       "6980  Navantia acaba de firmar con la Marina de Noru...   \n",
       "1589  Tienen una función parecida a los relojes o la...   \n",
       "240   En última instancia, cuando se apagan las luce...   \n",
       "7356  La compañía francesa Escape International ha p...   \n",
       "6763  El Gobierno busca que Bruselas apruebe un impu...   \n",
       "3341  comentarios 44Cybertron, el planeta de los Tra...   \n",
       "7826  La palabra bienquerencia es poco usada, pero e...   \n",
       "8578  Segunda victoria en el WRC para el finlandés E...   \n",
       "3456  No hay un equipo más desgraciado que el Almerí...   \n",
       "5659  Más allá de las cápsulas de café hay un mundo ...   \n",
       "2184  Evolucion de la vida sexual de la mujer. Fisio...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "6734  [5, 3270, 1058, 102, 1737, 4007, 240, 834, 10,...   \n",
       "1905  [34012, 12, 1, 1, 36, 46, 703, 58, 1, 36, 1652...   \n",
       "3049  [5, 14608, 14, 2248, 10396, 12, 2641, 10, 246,...   \n",
       "1772  [1, 54, 14, 1, 40, 554, 12, 102, 1, 1652, 12, ...   \n",
       "6980  [1, 11, 12, 23970, 58, 14, 27045, 12, 27768, 1...   \n",
       "1589  [440, 19, 2427, 14721, 40, 34, 14066, 96, 77, ...   \n",
       "240   [54, 1177, 5104, 206, 17, 16158, 77, 3867, 12,...   \n",
       "7356  [14, 1151, 2772, 7008, 29665, 260, 1048, 54, 1...   \n",
       "6763  [5, 3270, 232, 10, 14032, 36279, 102, 15003, 2...   \n",
       "3341  [7493, 1, 5, 3124, 12, 34, 8114, 5, 29619, 12,...   \n",
       "7826  [14, 203, 1, 4, 610, 32326, 403, 198, 11659, 2...   \n",
       "8578  [1090, 3982, 54, 5, 1, 240, 5, 19053, 1, 1, 10...   \n",
       "3456  [196, 124, 102, 4877, 130, 20027, 10, 5, 23674...   \n",
       "5659  [130, 219, 12, 77, 32819, 12, 10027, 124, 102,...   \n",
       "2184  [1, 12, 14, 195, 7773, 12, 14, 749, 1, 12, 14,...   \n",
       "\n",
       "                                          tokens_string     categoría  \\\n",
       "6734  el gobierno tiene un plan alternativo para ase...      politics   \n",
       "1905  prevalencia de [UNK] [UNK] y su relación con [...      medicine   \n",
       "3049  el sol la fuente inagotable de energía que da ...     astronomy   \n",
       "1772  [UNK] en la [UNK] a propósito de un [UNK] enfe...      medicine   \n",
       "6980  [UNK] acaba de firmar con la marina de noruega...      military   \n",
       "1589  tienen una función parecida a los relojes o la...          tech   \n",
       "240   en última instancia cuando se apagan las luces...          play   \n",
       "7356  la compañía francesa escape international ha p...      military   \n",
       "6763  el gobierno busca que bruselas apruebe un impu...      politics   \n",
       "3341  comentarios [UNK] el planeta de los transforme...     astronomy   \n",
       "7826  la palabra [UNK] es poco usada pero existe ind...      religion   \n",
       "8578  segunda victoria en el [UNK] para el finlandés...         motor   \n",
       "3456  no hay un equipo más desgraciado que el almerí...         sport   \n",
       "5659  más allá de las cápsulas de café hay un mundo ...  alimentation   \n",
       "2184  [UNK] de la vida sexual de la mujer [UNK] de l...      medicine   \n",
       "\n",
       "        predicción  \n",
       "6734      politics  \n",
       "1905      medicine  \n",
       "3049     astronomy  \n",
       "1772      medicine  \n",
       "6980      military  \n",
       "1589          tech  \n",
       "240           play  \n",
       "7356      military  \n",
       "6763      politics  \n",
       "3341     astronomy  \n",
       "7826      religion  \n",
       "8578         motor  \n",
       "3456         sport  \n",
       "5659  alimentation  \n",
       "2184      medicine  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_indices = test_dataset.indices\n",
    "df = pd.DataFrame(data={\n",
    "    \"texto\": dataset[test_indices]['text'],\n",
    "    \"tokens\": [tokenize_text(v) for v in dataset[test_indices]['text']],\n",
    "    \"categoría\": dataset[test_indices]['category'],\n",
    "    'predicción': predictions\n",
    "}, index=test_indices)\n",
    "\n",
    "df['tokens_string'] = df.tokens.apply(lambda t: ' '.join([id_2_token[i] for i in t]))\n",
    "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categoría\", \"predicción\"]]\n",
    "df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
    "    ]\n",
    ")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c5085d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categoría</th>\n",
       "      <th>predicción</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>Pues parece que la policía holandesa se precip...</td>\n",
       "      <td>[2020, 491, 10, 14, 1497, 38011, 17, 38012, 10...</td>\n",
       "      <td>pues parece que la policía holandesa se precip...</td>\n",
       "      <td>tech</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>El nuevo Consejo de Administración de Fincanti...</td>\n",
       "      <td>[5, 8, 17379, 12, 444, 12, 1, 1, 29649, 45, 14...</td>\n",
       "      <td>el nuevo consejo de administración de [UNK] [U...</td>\n",
       "      <td>military</td>\n",
       "      <td>economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>Fuente de la imagen, OtherJune Williams creció...</td>\n",
       "      <td>[2248, 12, 14, 564, 1, 10953, 5680, 57, 40, 34...</td>\n",
       "      <td>fuente de la imagen [UNK] williams creció junt...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>El Elche espera a Óscar Plano. El delantero ma...</td>\n",
       "      <td>[5, 36649, 4040, 40, 2947, 1846, 5, 1, 8710, 3...</td>\n",
       "      <td>el elche espera a óscar plano el [UNK] madrile...</td>\n",
       "      <td>sport</td>\n",
       "      <td>economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>Algunos de los monumentos más famosos del Rein...</td>\n",
       "      <td>[501, 12, 34, 23928, 130, 1018, 7, 2125, 8269,...</td>\n",
       "      <td>algunos de los monumentos más famosos del rein...</td>\n",
       "      <td>play</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>Hay anuncios que irremediablemente se quedan a...</td>\n",
       "      <td>[124, 11554, 10, 12505, 17, 1895, 12251, 54, 2...</td>\n",
       "      <td>hay anuncios que irremediablemente se quedan a...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>Navantia junto a otras 5 grandes empresas, Rep...</td>\n",
       "      <td>[1, 57, 40, 2255, 901, 8594, 1, 1, 14784, 3176...</td>\n",
       "      <td>[UNK] junto a otras grandes empresas [UNK] [UN...</td>\n",
       "      <td>military</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>La versión más accesible del Lucid Air llega p...</td>\n",
       "      <td>[14, 2049, 130, 14266, 7, 1, 7550, 333, 45, 31...</td>\n",
       "      <td>la versión más accesible del [UNK] air llega p...</td>\n",
       "      <td>motor</td>\n",
       "      <td>economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>Pedro Sánchez ha regalado toda una ley de adoc...</td>\n",
       "      <td>[399, 9554, 260, 19502, 89, 19, 2565, 12, 1, 5...</td>\n",
       "      <td>pedro sánchez ha regalado toda una ley de [UNK...</td>\n",
       "      <td>religion</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8103</th>\n",
       "      <td>Es algo que está sorprendiendo en la red. Mill...</td>\n",
       "      <td>[4, 559, 10, 482, 24341, 54, 14, 1063, 1059, 1...</td>\n",
       "      <td>es algo que está sorprendiendo en la red millo...</td>\n",
       "      <td>religion</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>La Edad Media no fue la época maloliente y suc...</td>\n",
       "      <td>[14, 7466, 4151, 196, 395, 14, 1629, 1, 36, 47...</td>\n",
       "      <td>la edad media no fue la época [UNK] y sucia qu...</td>\n",
       "      <td>economy</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>La última generación de videoconsolas lanzadas...</td>\n",
       "      <td>[14, 1177, 2224, 12, 30807, 21743, 269, 1481, ...</td>\n",
       "      <td>la última generación de videoconsolas lanzadas...</td>\n",
       "      <td>play</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9576</th>\n",
       "      <td>La necesidad de energía está presente en práct...</td>\n",
       "      <td>[14, 2497, 12, 2641, 482, 388, 54, 4057, 201, ...</td>\n",
       "      <td>la necesidad de energía está presente en práct...</td>\n",
       "      <td>economy</td>\n",
       "      <td>fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>¿Qué es la farmacología? Ciencia que estudia l...</td>\n",
       "      <td>[1183, 4, 14, 1, 4182, 10, 38159, 34, 11561, 1...</td>\n",
       "      <td>qué es la [UNK] ciencia que estudia los fármac...</td>\n",
       "      <td>medicine</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>El Ayuntamiento de Palma ha colocado este mart...</td>\n",
       "      <td>[5, 4852, 12, 9239, 260, 15095, 55, 9247, 14, ...</td>\n",
       "      <td>el ayuntamiento de palma ha colocado este mart...</td>\n",
       "      <td>religion</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "1120  Pues parece que la policía holandesa se precip...   \n",
       "7473  El nuevo Consejo de Administración de Fincanti...   \n",
       "2765  Fuente de la imagen, OtherJune Williams creció...   \n",
       "4142  El Elche espera a Óscar Plano. El delantero ma...   \n",
       "689   Algunos de los monumentos más famosos del Rein...   \n",
       "5835  Hay anuncios que irremediablemente se quedan a...   \n",
       "7368  Navantia junto a otras 5 grandes empresas, Rep...   \n",
       "9001  La versión más accesible del Lucid Air llega p...   \n",
       "7846  Pedro Sánchez ha regalado toda una ley de adoc...   \n",
       "8103  Es algo que está sorprendiendo en la red. Mill...   \n",
       "9900  La Edad Media no fue la época maloliente y suc...   \n",
       "786   La última generación de videoconsolas lanzadas...   \n",
       "9576  La necesidad de energía está presente en práct...   \n",
       "2098  ¿Qué es la farmacología? Ciencia que estudia l...   \n",
       "7888  El Ayuntamiento de Palma ha colocado este mart...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1120  [2020, 491, 10, 14, 1497, 38011, 17, 38012, 10...   \n",
       "7473  [5, 8, 17379, 12, 444, 12, 1, 1, 29649, 45, 14...   \n",
       "2765  [2248, 12, 14, 564, 1, 10953, 5680, 57, 40, 34...   \n",
       "4142  [5, 36649, 4040, 40, 2947, 1846, 5, 1, 8710, 3...   \n",
       "689   [501, 12, 34, 23928, 130, 1018, 7, 2125, 8269,...   \n",
       "5835  [124, 11554, 10, 12505, 17, 1895, 12251, 54, 2...   \n",
       "7368  [1, 57, 40, 2255, 901, 8594, 1, 1, 14784, 3176...   \n",
       "9001  [14, 2049, 130, 14266, 7, 1, 7550, 333, 45, 31...   \n",
       "7846  [399, 9554, 260, 19502, 89, 19, 2565, 12, 1, 5...   \n",
       "8103  [4, 559, 10, 482, 24341, 54, 14, 1063, 1059, 1...   \n",
       "9900  [14, 7466, 4151, 196, 395, 14, 1629, 1, 36, 47...   \n",
       "786   [14, 1177, 2224, 12, 30807, 21743, 269, 1481, ...   \n",
       "9576  [14, 2497, 12, 2641, 482, 388, 54, 4057, 201, ...   \n",
       "2098  [1183, 4, 14, 1, 4182, 10, 38159, 34, 11561, 1...   \n",
       "7888  [5, 4852, 12, 9239, 260, 15095, 55, 9247, 14, ...   \n",
       "\n",
       "                                          tokens_string     categoría  \\\n",
       "1120  pues parece que la policía holandesa se precip...          tech   \n",
       "7473  el nuevo consejo de administración de [UNK] [U...      military   \n",
       "2765  fuente de la imagen [UNK] williams creció junt...     astronomy   \n",
       "4142  el elche espera a óscar plano el [UNK] madrile...         sport   \n",
       "689   algunos de los monumentos más famosos del rein...          play   \n",
       "5835  hay anuncios que irremediablemente se quedan a...  alimentation   \n",
       "7368  [UNK] junto a otras grandes empresas [UNK] [UN...      military   \n",
       "9001  la versión más accesible del [UNK] air llega p...         motor   \n",
       "7846  pedro sánchez ha regalado toda una ley de [UNK...      religion   \n",
       "8103  es algo que está sorprendiendo en la red millo...      religion   \n",
       "9900  la edad media no fue la época [UNK] y sucia qu...       economy   \n",
       "786   la última generación de videoconsolas lanzadas...          play   \n",
       "9576  la necesidad de energía está presente en práct...       economy   \n",
       "2098  qué es la [UNK] ciencia que estudia los fármac...      medicine   \n",
       "7888  el ayuntamiento de palma ha colocado este mart...      religion   \n",
       "\n",
       "     predicción  \n",
       "1120       play  \n",
       "7473    economy  \n",
       "2765   religion  \n",
       "4142    economy  \n",
       "689        tech  \n",
       "5835  astronomy  \n",
       "7368      motor  \n",
       "9001    economy  \n",
       "7846   politics  \n",
       "8103  astronomy  \n",
       "9900  astronomy  \n",
       "786        tech  \n",
       "9576    fashion  \n",
       "2098   military  \n",
       "7888   politics  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = df[df['categoría'] != df['predicción']]\n",
    "errors.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88103e5e",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- En este caso tenemos una tarea de clasificación de texto de múltiples clases.\n",
    "- Estamos usando un bloque LSTM como featurizer, es decir lo usamos para extraer features de las secuencias de entrada con las cuales harémos predicciones luego.\n",
    "- Nótese que de las capas LSTM, solo nos interesa la última, ya que esta recupera todas las operaciones enalazadas anteriores.\n",
    "- Observamos que el modelo toma su tiempo en entrenar, esto es natural debido al diseño de las LSTM, donde por cada paso de tiempo se debe computar un gradiente, por lo que el computo es mucho mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73526f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
